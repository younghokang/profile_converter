{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Project Structure and Dependencies",
        "description": "Initialize the project repository with proper structure and install required dependencies including python-docx for Word file parsing.",
        "details": "1. Create a new Python project with the following structure:\n```\nresume_parser/\n├── src/\n│   ├── __init__.py\n│   ├── parser/\n│   │   ├── __init__.py\n│   │   └── docx_parser.py\n│   ├── converter/\n│   │   ├── __init__.py\n│   │   └── json_converter.py\n│   └── cli/\n│       ├── __init__.py\n│       └── main.py\n├── tests/\n│   ├── __init__.py\n│   ├── test_parser.py\n│   └── test_converter.py\n├── requirements.txt\n├── setup.py\n└── README.md\n```\n2. Create requirements.txt with necessary dependencies:\n```\npython-docx>=0.8.11\nclick>=8.0.0\n```\n3. Setup virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -r requirements.txt\n```\n4. Initialize git repository with appropriate .gitignore file",
        "testStrategy": "Verify project structure is correctly set up and all dependencies can be installed without errors. Run a simple test script to confirm python-docx can be imported and used.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Define Standard Resume JSON Schema",
        "description": "Create a standardized JSON schema that will be used as the output format for parsed resumes, including fields for personal information, education, experience, skills, etc.",
        "details": "1. Create a schema definition file in `src/converter/schema.py`\n2. Define the JSON schema with the following structure:\n```python\nRESUME_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"personal_info\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": {\"type\": \"string\"},\n                \"contact\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"email\": {\"type\": \"string\"},\n                        \"phone\": {\"type\": \"string\"},\n                        \"address\": {\"type\": \"string\"}\n                    }\n                }\n            }\n        },\n        \"education\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"institution\": {\"type\": \"string\"},\n                    \"degree\": {\"type\": \"string\"},\n                    \"field\": {\"type\": \"string\"},\n                    \"start_date\": {\"type\": \"string\"},\n                    \"end_date\": {\"type\": \"string\"},\n                    \"description\": {\"type\": \"string\"}\n                }\n            }\n        },\n        \"experience\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"company\": {\"type\": \"string\"},\n                    \"position\": {\"type\": \"string\"},\n                    \"start_date\": {\"type\": \"string\"},\n                    \"end_date\": {\"type\": \"string\"},\n                    \"description\": {\"type\": \"string\"}\n                }\n            }\n        },\n        \"skills\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"}\n        },\n        \"certifications\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"issuer\": {\"type\": \"string\"},\n                    \"date\": {\"type\": \"string\"}\n                }\n            }\n        },\n        \"languages\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"language\": {\"type\": \"string\"},\n                    \"proficiency\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }\n}\n```\n3. Create a sample JSON output file that conforms to this schema for testing and documentation purposes",
        "testStrategy": "Create unit tests to validate that the schema is properly defined and that sample data can be validated against it. Test edge cases like missing fields and different data types.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement Word Document Parser",
        "description": "Develop a module to parse Word (.docx) files and extract text and table content using the python-docx library.",
        "details": "1. In `src/parser/docx_parser.py`, implement a DocxParser class:\n```python\nfrom docx import Document\n\nclass DocxParser:\n    def __init__(self, file_path):\n        self.file_path = file_path\n        self.document = None\n        \n    def load_document(self):\n        \"\"\"Load the Word document from the file path.\"\"\"\n        try:\n            self.document = Document(self.file_path)\n            return True\n        except Exception as e:\n            print(f\"Error loading document: {e}\")\n            return False\n    \n    def extract_text(self):\n        \"\"\"Extract all text from paragraphs in the document.\"\"\"\n        if not self.document:\n            if not self.load_document():\n                return []\n        \n        return [p.text for p in self.document.paragraphs if p.text.strip()]\n    \n    def extract_tables(self):\n        \"\"\"Extract all tables from the document.\"\"\"\n        if not self.document:\n            if not self.load_document():\n                return []\n        \n        tables_data = []\n        for table in self.document.tables:\n            table_data = []\n            for row in table.rows:\n                row_data = [cell.text for cell in row.cells]\n                table_data.append(row_data)\n            tables_data.append(table_data)\n        \n        return tables_data\n    \n    def extract_all_content(self):\n        \"\"\"Extract both text and tables from the document.\"\"\"\n        return {\n            \"text\": self.extract_text(),\n            \"tables\": self.extract_tables()\n        }\n```\n2. Add error handling for different document formats and corrupted files\n3. Implement methods to handle document sections and formatting if needed",
        "testStrategy": "Create unit tests with sample Word documents containing various layouts, tables, and text formats. Test the parser with both well-structured and poorly structured documents to ensure robust extraction.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Develop JSON Converter Module",
        "description": "Create a module that converts the extracted document content into the standardized JSON format according to the defined schema.",
        "details": "1. In `src/converter/json_converter.py`, implement a ResumeConverter class:\n```python\nimport re\nimport json\nfrom ..parser.docx_parser import DocxParser\n\nclass ResumeConverter:\n    def __init__(self, parser_output):\n        self.content = parser_output\n        self.resume_data = {\n            \"personal_info\": {\"name\": \"\", \"contact\": {\"email\": \"\", \"phone\": \"\", \"address\": \"\"}},\n            \"education\": [],\n            \"experience\": [],\n            \"skills\": [],\n            \"certifications\": [],\n            \"languages\": []\n        }\n    \n    def extract_personal_info(self):\n        \"\"\"Extract name and contact information from the content.\"\"\"\n        text_content = self.content.get(\"text\", [])\n        \n        # Simple heuristic: First line might be the name\n        if text_content and not self.resume_data[\"personal_info\"][\"name\"]:\n            self.resume_data[\"personal_info\"][\"name\"] = text_content[0]\n        \n        # Look for email addresses\n        email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}'\n        for line in text_content:\n            email_match = re.search(email_pattern, line)\n            if email_match and not self.resume_data[\"personal_info\"][\"contact\"][\"email\"]:\n                self.resume_data[\"personal_info\"][\"contact\"][\"email\"] = email_match.group(0)\n        \n        # Look for phone numbers (simplified pattern)\n        phone_pattern = r'\\+?\\d[\\d\\s-]{8,}'\n        for line in text_content:\n            phone_match = re.search(phone_pattern, line)\n            if phone_match and not self.resume_data[\"personal_info\"][\"contact\"][\"phone\"]:\n                self.resume_data[\"personal_info\"][\"contact\"][\"phone\"] = phone_match.group(0)\n    \n    def extract_education(self):\n        \"\"\"Extract education information from the content.\"\"\"\n        # Implementation depends on resume format, but could look for keywords\n        # like 'education', 'university', 'college', 'degree', etc.\n        # This is a simplified example\n        text_content = self.content.get(\"text\", [])\n        education_section = False\n        current_education = {}\n        \n        for line in text_content:\n            if re.search(r'education|academic', line.lower()):\n                education_section = True\n                continue\n            \n            if education_section and line.strip():\n                if re.search(r'university|college|school', line.lower()) and current_education:\n                    self.resume_data[\"education\"].append(current_education)\n                    current_education = {}\n                \n                if not current_education.get(\"institution\") and re.search(r'university|college|school', line.lower()):\n                    current_education[\"institution\"] = line.strip()\n                elif not current_education.get(\"degree\") and re.search(r'bachelor|master|phd|diploma', line.lower()):\n                    current_education[\"degree\"] = line.strip()\n                # Add more conditions for other education fields\n        \n        if current_education:\n            self.resume_data[\"education\"].append(current_education)\n    \n    def extract_experience(self):\n        \"\"\"Extract work experience information from the content.\"\"\"\n        # Similar approach to education extraction\n        # Look for sections with keywords like 'experience', 'work', 'employment'\n        pass\n    \n    def extract_skills(self):\n        \"\"\"Extract skills from the content.\"\"\"\n        # Look for skills section and extract individual skills\n        pass\n    \n    def convert(self):\n        \"\"\"Convert the parsed content to the standard JSON format.\"\"\"\n        self.extract_personal_info()\n        self.extract_education()\n        self.extract_experience()\n        self.extract_skills()\n        # Add more extraction methods as needed\n        \n        return self.resume_data\n    \n    def to_json(self):\n        \"\"\"Return the resume data as a JSON string.\"\"\"\n        return json.dumps(self.convert(), indent=2)\n```\n2. Implement more sophisticated extraction methods for each section of the resume\n3. Add heuristics to identify section boundaries in unstructured text\n4. Implement table parsing logic to extract structured data from tables",
        "testStrategy": "Create unit tests with various sample resume content and verify that the converter correctly identifies and extracts information into the proper JSON structure. Test with different formats and layouts to ensure robustness.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement CLI Interface",
        "description": "Create a command-line interface that allows users to input a Word document and receive the parsed JSON output.",
        "details": "1. In `src/cli/main.py`, implement a CLI using the Click library:\n```python\nimport os\nimport json\nimport click\nfrom ..parser.docx_parser import DocxParser\nfrom ..converter.json_converter import ResumeConverter\n\n@click.group()\ndef cli():\n    \"\"\"Resume Parser: Convert Word resumes to standardized JSON format.\"\"\"\n    pass\n\n@cli.command()\n@click.argument('input_file', type=click.Path(exists=True))\n@click.option('--output', '-o', type=click.Path(), help='Output JSON file path')\n@click.option('--pretty/--no-pretty', default=True, help='Pretty print the JSON output')\ndef parse(input_file, output, pretty):\n    \"\"\"Parse a Word resume file and convert it to JSON.\"\"\"\n    click.echo(f\"Parsing resume: {input_file}\")\n    \n    # Parse the document\n    parser = DocxParser(input_file)\n    content = parser.extract_all_content()\n    \n    # Convert to JSON\n    converter = ResumeConverter(content)\n    resume_json = converter.convert()\n    \n    # Output handling\n    if output:\n        with open(output, 'w', encoding='utf-8') as f:\n            json.dump(resume_json, f, indent=4 if pretty else None)\n        click.echo(f\"JSON output saved to: {output}\")\n    else:\n        # Print to console\n        formatted_json = json.dumps(resume_json, indent=4 if pretty else None)\n        click.echo(formatted_json)\n\n@cli.command()\ndef version():\n    \"\"\"Show the version of the resume parser.\"\"\"\n    click.echo(\"Resume Parser v0.1.0\")\n\nif __name__ == '__main__':\n    cli()\n```\n2. Create an entry point in setup.py to make the CLI globally accessible\n3. Add help text and examples for each command\n4. Implement error handling and user-friendly error messages",
        "testStrategy": "Test the CLI with various input files and options. Verify that the CLI correctly handles valid inputs, produces expected outputs, and provides helpful error messages for invalid inputs.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Enhance Resume Section Recognition",
        "description": "Improve the parser's ability to recognize different sections of a resume (education, experience, skills, etc.) regardless of formatting or layout.",
        "details": "1. Implement a SectionRecognizer class in `src/parser/section_recognizer.py`:\n```python\nimport re\n\nclass SectionRecognizer:\n    \"\"\"Recognizes and extracts sections from resume content.\"\"\"\n    \n    # Common section headers in resumes\n    SECTION_KEYWORDS = {\n        'personal_info': ['personal information', 'personal details', 'contact'],\n        'education': ['education', 'academic background', 'qualifications', 'academic'],\n        'experience': ['experience', 'work experience', 'employment history', 'work history', 'professional experience'],\n        'skills': ['skills', 'technical skills', 'competencies', 'expertise'],\n        'certifications': ['certifications', 'certificates', 'professional certifications'],\n        'languages': ['languages', 'language proficiency'],\n        'projects': ['projects', 'personal projects', 'professional projects'],\n        'summary': ['summary', 'professional summary', 'profile', 'about me']\n    }\n    \n    def __init__(self, text_content):\n        self.text_content = text_content\n        self.sections = {}\n    \n    def identify_sections(self):\n        \"\"\"Identify the different sections in the resume.\"\"\"\n        current_section = None\n        section_content = []\n        \n        for line in self.text_content:\n            line = line.strip()\n            if not line:\n                continue\n            \n            # Check if this line is a section header\n            new_section = self._is_section_header(line)\n            \n            if new_section:\n                # Save the previous section if it exists\n                if current_section:\n                    self.sections[current_section] = section_content\n                \n                # Start a new section\n                current_section = new_section\n                section_content = []\n            elif current_section:\n                # Add content to the current section\n                section_content.append(line)\n        \n        # Add the last section\n        if current_section and section_content:\n            self.sections[current_section] = section_content\n        \n        return self.sections\n    \n    def _is_section_header(self, line):\n        \"\"\"Check if a line is a section header and return the section type.\"\"\"\n        line_lower = line.lower()\n        \n        # Check for common section headers\n        for section, keywords in self.SECTION_KEYWORDS.items():\n            for keyword in keywords:\n                # Check if the keyword is the entire line or a significant part\n                if keyword == line_lower or \\\n                   (len(line_lower) < 30 and keyword in line_lower and \\\n                    (line_lower.startswith(keyword) or \\\n                     re.match(r'^[\\W]*' + re.escape(keyword) + r'[\\W]*$', line_lower))):\n                    return section\n        \n        return None\n    \n    def get_section(self, section_name):\n        \"\"\"Get the content of a specific section.\"\"\"\n        return self.sections.get(section_name, [])\n```\n2. Integrate the SectionRecognizer with the ResumeConverter to improve section identification\n3. Add support for custom section recognition based on formatting (bold headers, larger font, etc.)\n4. Implement fuzzy matching for section headers to handle variations",
        "testStrategy": "Test with a diverse set of resume formats and layouts. Verify that the section recognizer correctly identifies standard sections across different resume styles. Create edge cases with unusual section names or formatting.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Table Data Extraction",
        "description": "Enhance the parser to extract and interpret data from tables in the resume, which often contain structured information like education or work experience.",
        "details": "1. Create a TableDataExtractor class in `src/parser/table_extractor.py`:\n```python\nclass TableDataExtractor:\n    \"\"\"Extracts and interprets data from tables in resumes.\"\"\"\n    \n    def __init__(self, tables_data):\n        self.tables_data = tables_data\n    \n    def extract_education_tables(self):\n        \"\"\"Extract education information from tables.\"\"\"\n        education_entries = []\n        \n        for table in self.tables_data:\n            # Skip empty tables\n            if not table or not table[0]:\n                continue\n            \n            # Check if this might be an education table\n            header_row = [cell.lower() for cell in table[0]]\n            education_keywords = ['degree', 'institution', 'university', 'education', 'qualification']\n            \n            if any(keyword in ' '.join(header_row) for keyword in education_keywords):\n                # This looks like an education table\n                for row_idx in range(1, len(table)):\n                    row = table[row_idx]\n                    if not any(cell.strip() for cell in row):\n                        continue  # Skip empty rows\n                    \n                    education_entry = {}\n                    \n                    # Map columns based on header\n                    for col_idx, header in enumerate(header_row):\n                        if col_idx < len(row):\n                            value = row[col_idx].strip()\n                            if not value:\n                                continue\n                                \n                            if 'institution' in header or 'university' in header or 'school' in header:\n                                education_entry['institution'] = value\n                            elif 'degree' in header or 'qualification' in header:\n                                education_entry['degree'] = value\n                            elif 'field' in header or 'major' in header or 'subject' in header:\n                                education_entry['field'] = value\n                            elif 'start' in header or 'from' in header:\n                                education_entry['start_date'] = value\n                            elif 'end' in header or 'to' in header or 'completion' in header:\n                                education_entry['end_date'] = value\n                            elif 'grade' in header or 'gpa' in header or 'result' in header:\n                                education_entry['description'] = f\"Grade: {value}\"\n                    \n                    if education_entry:\n                        education_entries.append(education_entry)\n        \n        return education_entries\n    \n    def extract_experience_tables(self):\n        \"\"\"Extract work experience information from tables.\"\"\"\n        experience_entries = []\n        \n        for table in self.tables_data:\n            # Similar approach to education tables\n            # Look for experience-related headers\n            # Map columns to experience fields\n            pass\n        \n        return experience_entries\n    \n    def extract_skills_tables(self):\n        \"\"\"Extract skills information from tables.\"\"\"\n        skills = []\n        \n        for table in self.tables_data:\n            # Look for skills tables and extract skills\n            pass\n        \n        return skills\n```\n2. Integrate the TableDataExtractor with the ResumeConverter\n3. Add heuristics to determine table purpose based on content\n4. Implement handling for merged cells and complex table layouts",
        "testStrategy": "Create test cases with resumes containing various table formats. Verify that the extractor correctly identifies table types and extracts structured data. Test with complex tables, merged cells, and tables without clear headers.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Exception Handling and Validation",
        "description": "Add robust exception handling and validation to ensure the parser can handle a wide variety of document formats and content without crashing.",
        "details": "1. Create a validation module in `src/utils/validation.py`:\n```python\nimport os\nimport json\nfrom jsonschema import validate, ValidationError\nfrom ..converter.schema import RESUME_SCHEMA\n\ndef validate_file_extension(file_path, allowed_extensions=None):\n    \"\"\"Validate that the file has an allowed extension.\"\"\"\n    if allowed_extensions is None:\n        allowed_extensions = ['.docx']\n    \n    _, ext = os.path.splitext(file_path)\n    if ext.lower() not in allowed_extensions:\n        raise ValueError(f\"Unsupported file extension: {ext}. Allowed extensions: {', '.join(allowed_extensions)}\")\n    \n    return True\n\ndef validate_file_exists(file_path):\n    \"\"\"Validate that the file exists and is readable.\"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n    \n    if not os.path.isfile(file_path):\n        raise IsADirectoryError(f\"Expected a file, got a directory: {file_path}\")\n    \n    if not os.access(file_path, os.R_OK):\n        raise PermissionError(f\"Permission denied: {file_path}\")\n    \n    return True\n\ndef validate_json_output(json_data):\n    \"\"\"Validate that the JSON output conforms to the schema.\"\"\"\n    try:\n        validate(instance=json_data, schema=RESUME_SCHEMA)\n        return True\n    except ValidationError as e:\n        raise ValidationError(f\"JSON validation failed: {e}\")\n\ndef sanitize_output(json_data):\n    \"\"\"Sanitize the output to ensure it's valid JSON and remove any problematic characters.\"\"\"\n    # Remove any control characters that might cause JSON parsing issues\n    if isinstance(json_data, dict):\n        for key, value in json_data.items():\n            if isinstance(value, str):\n                json_data[key] = ''.join(c for c in value if ord(c) >= 32)\n            elif isinstance(value, (dict, list)):\n                json_data[key] = sanitize_output(value)\n    elif isinstance(json_data, list):\n        for i, item in enumerate(json_data):\n            if isinstance(item, str):\n                json_data[i] = ''.join(c for c in item if ord(c) >= 32)\n            elif isinstance(item, (dict, list)):\n                json_data[i] = sanitize_output(item)\n    \n    return json_data\n```\n2. Add try-except blocks in all critical functions\n3. Implement graceful degradation for parsing errors\n4. Add logging for errors and warnings\n5. Update the CLI to handle and report errors in a user-friendly way",
        "testStrategy": "Create test cases with invalid inputs, corrupted files, and edge cases. Verify that the application handles errors gracefully and provides helpful error messages. Test validation functions with both valid and invalid data.",
        "priority": "medium",
        "dependencies": [
          3,
          4,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Create Simple Web UI (Optional)",
        "description": "Develop a simple web interface that allows users to upload Word documents and download the resulting JSON.",
        "details": "1. Create a Flask application in `src/web/app.py`:\n```python\nimport os\nimport json\nfrom flask import Flask, request, render_template, jsonify, send_file\nfrom werkzeug.utils import secure_filename\nfrom ..parser.docx_parser import DocxParser\nfrom ..converter.json_converter import ResumeConverter\nfrom ..utils.validation import validate_file_extension, sanitize_output\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = 'uploads'\napp.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size\n\n# Ensure upload directory exists\nos.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part'}), 400\n    \n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n    \n    try:\n        # Validate file extension\n        validate_file_extension(file.filename)\n        \n        # Save the file\n        filename = secure_filename(file.filename)\n        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n        file.save(file_path)\n        \n        # Parse the document\n        parser = DocxParser(file_path)\n        content = parser.extract_all_content()\n        \n        # Convert to JSON\n        converter = ResumeConverter(content)\n        resume_json = converter.convert()\n        \n        # Sanitize output\n        resume_json = sanitize_output(resume_json)\n        \n        # Save JSON to file\n        json_filename = f\"{os.path.splitext(filename)[0]}.json\"\n        json_path = os.path.join(app.config['UPLOAD_FOLDER'], json_filename)\n        with open(json_path, 'w', encoding='utf-8') as f:\n            json.dump(resume_json, f, indent=4)\n        \n        return jsonify({\n            'success': True,\n            'message': 'File successfully processed',\n            'json_file': json_filename,\n            'resume_data': resume_json\n        })\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 400\n\n@app.route('/download/<filename>')\ndef download_file(filename):\n    return send_file(\n        os.path.join(app.config['UPLOAD_FOLDER'], filename),\n        as_attachment=True,\n        download_name=filename\n    )\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n2. Create HTML templates in `src/web/templates/`:\n   - index.html with a file upload form\n   - result display with JSON preview and download option\n3. Add static CSS and JavaScript for a better user experience\n4. Implement file cleanup to remove uploaded files after processing",
        "testStrategy": "Test the web UI with various browsers and devices. Verify that file uploads work correctly, error messages are displayed properly, and JSON downloads function as expected. Test with large files and concurrent users.",
        "priority": "low",
        "dependencies": [
          3,
          4,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Write Documentation and Create Sample Files",
        "description": "Create comprehensive documentation for the project, including installation instructions, usage examples, and sample resume files for testing.",
        "details": "1. Update the README.md with the following sections:\n   - Project overview and purpose\n   - Installation instructions\n   - Usage examples (CLI and web UI if implemented)\n   - JSON schema documentation\n   - Troubleshooting guide\n2. Create a docs/ directory with detailed documentation:\n   - Architecture overview\n   - Module descriptions\n   - Extension points for future development\n3. Create sample/ directory with:\n   - Example Word resume files in various formats\n   - Expected JSON output for each sample\n4. Add inline code documentation and docstrings\n5. Create a CONTRIBUTING.md file with guidelines for contributors",
        "testStrategy": "Review documentation for clarity and completeness. Test installation and usage instructions on a fresh environment to ensure they work as described. Verify that sample files can be processed correctly using the documented commands.",
        "priority": "medium",
        "dependencies": [
          5,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Design and Test Korean Resume Section Recognition Rules",
        "description": "Analyze various Korean resume samples to design and test rules for identifying key sections such as name, contact information, education, work experience, and skills using regex patterns and keyword-based approaches.",
        "details": "1. Create a specialized Korean resume section recognizer in `src/parser/korean_section_recognizer.py`:\n```python\nimport re\nfrom .section_recognizer import SectionRecognizer\n\nclass KoreanSectionRecognizer(SectionRecognizer):\n    \"\"\"Specialized section recognizer for Korean resumes.\"\"\"\n    \n    # Korean section keywords dictionary\n    KOREAN_SECTION_KEYWORDS = {\n        'personal_info': ['개인정보', '인적사항', '신상정보', '기본정보'],\n        'education': ['학력', '학력사항', '교육', '학교', '교육이력'],\n        'experience': ['경력', '경력사항', '직장경력', '업무경험', '경험', '직무경험'],\n        'skills': ['기술', '기술스택', '보유기술', '역량', '전문성', '기술역량'],\n        'certifications': ['자격증', '자격', '보유자격증', '자격사항'],\n        'projects': ['프로젝트', '수행 프로젝트', '프로젝트 경험'],\n        'languages': ['어학', '외국어', '언어능력', '어학능력'],\n        'awards': ['수상', '수상경력', '수상내역'],\n        'additional': ['기타', '추가사항', '부가정보']\n    }\n    \n    # Regex patterns for Korean resume sections\n    SECTION_PATTERNS = {\n        'name': r'이름\\s*:?\\s*([가-힣]{2,4})',\n        'phone': r'(?:연락처|전화|휴대폰|전화번호|휴대전화)\\s*:?\\s*(01[016789][-\\s]?\\d{3,4}[-\\s]?\\d{4})',\n        'email': r'(?:이메일|메일|E-mail)\\s*:?\\s*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})',\n        'birth': r'(?:생년월일|생일|출생)\\s*:?\\s*(\\d{4}[-./년]?\\s*\\d{1,2}[-./월]?\\s*\\d{1,2}[일]?)',\n        'address': r'(?:주소|거주지)\\s*:?\\s*([가-힣0-9\\s-]+)'\n    }\n    \n    def __init__(self):\n        super().__init__()\n        self.section_keywords.update(self.KOREAN_SECTION_KEYWORDS)\n    \n    def identify_section_by_heading(self, text):\n        \"\"\"Identify section based on Korean headings.\"\"\"\n        text = text.strip()\n        \n        # Check for section headers with common formatting patterns\n        for section, keywords in self.KOREAN_SECTION_KEYWORDS.items():\n            for keyword in keywords:\n                # Check various heading patterns (e.g., \"학력\", \"[ 학력 ]\", \"■ 학력\")\n                patterns = [\n                    rf'^{keyword}$',\n                    rf'^[【\\[\\(]?\\s*{keyword}\\s*[\\]\\)】]?$',\n                    rf'^[■◆●◎◇□▣▶]\\s*{keyword}',\n                    rf'^{keyword}\\s*[:]?$'\n                ]\n                \n                for pattern in patterns:\n                    if re.search(pattern, text, re.IGNORECASE):\n                        return section\n        \n        return None\n    \n    def extract_personal_info(self, text):\n        \"\"\"Extract personal information using Korean-specific patterns.\"\"\"\n        info = {}\n        \n        for field, pattern in self.SECTION_PATTERNS.items():\n            match = re.search(pattern, text)\n            if match:\n                info[field] = match.group(1)\n                \n        return info\n    \n    def is_education_entry(self, text):\n        \"\"\"Check if text contains Korean education-related information.\"\"\"\n        education_indicators = [\n            r'대학교', r'고등학교', r'학교', r'전공', r'학사', r'석사', r'박사',\n            r'졸업', r'재학', r'수료', r'학위', r'\\d{4}년\\s*\\d{1,2}월'\n        ]\n        \n        for indicator in education_indicators:\n            if re.search(indicator, text):\n                return True\n        return False\n    \n    def is_experience_entry(self, text):\n        \"\"\"Check if text contains Korean work experience information.\"\"\"\n        experience_indicators = [\n            r'회사', r'직장', r'직위', r'직책', r'업무', r'담당', r'근무',\n            r'\\d{4}년\\s*\\d{1,2}월\\s*[~-]\\s*(?:\\d{4}년\\s*\\d{1,2}월|현재)'\n        ]\n        \n        for indicator in experience_indicators:\n            if re.search(indicator, text):\n                return True\n        return False\n```\n\n2. Update the main section recognizer to incorporate Korean language support:\n```python\n# In src/parser/section_recognizer.py\n\ndef detect_language(self, text):\n    \"\"\"Detect if the resume is primarily in Korean or English.\"\"\"\n    # Simple heuristic: count Korean characters vs English characters\n    korean_chars = len(re.findall(r'[가-힣]', text))\n    english_chars = len(re.findall(r'[a-zA-Z]', text))\n    \n    return 'korean' if korean_chars > english_chars else 'english'\n\ndef get_appropriate_recognizer(self, text):\n    \"\"\"Return the appropriate language-specific recognizer.\"\"\"\n    language = self.detect_language(text)\n    if language == 'korean':\n        return KoreanSectionRecognizer()\n    return self  # Default English recognizer\n```\n\n3. Create a test module for Korean resume section recognition in `tests/test_korean_recognition.py`:\n```python\nimport unittest\nfrom src.parser.korean_section_recognizer import KoreanSectionRecognizer\n\nclass TestKoreanSectionRecognition(unittest.TestCase):\n    \n    def setUp(self):\n        self.recognizer = KoreanSectionRecognizer()\n        \n    def test_section_identification(self):\n        test_cases = [\n            (\"학력\", \"education\"),\n            (\"[ 학력 사항 ]\", \"education\"),\n            (\"■ 경력\", \"experience\"),\n            (\"기술 스택:\", \"skills\"),\n            (\"자격증\", \"certifications\"),\n            (\"프로젝트 경험\", \"projects\")\n        ]\n        \n        for heading, expected in test_cases:\n            self.assertEqual(self.recognizer.identify_section_by_heading(heading), expected)\n    \n    def test_personal_info_extraction(self):\n        text = \"\"\"\n        이름: 홍길동\n        연락처: 010-1234-5678\n        이메일: hong@example.com\n        생년월일: 1990년 1월 1일\n        주소: 서울시 강남구\n        \"\"\"\n        \n        info = self.recognizer.extract_personal_info(text)\n        self.assertEqual(info['name'], '홍길동')\n        self.assertEqual(info['phone'], '010-1234-5678')\n        self.assertEqual(info['email'], 'hong@example.com')\n        \n    def test_education_detection(self):\n        education_texts = [\n            \"서울대학교 컴퓨터공학과 졸업 (2015년 2월)\",\n            \"한국고등학교 (2010년 2월 졸업)\",\n            \"컴퓨터공학 석사 (2017년)\"\n        ]\n        \n        for text in education_texts:\n            self.assertTrue(self.recognizer.is_education_entry(text))\n            \n    def test_experience_detection(self):\n        experience_texts = [\n            \"ABC 회사 개발팀 (2018년 3월 ~ 현재)\",\n            \"XYZ 주식회사 선임연구원 (2015년 5월 ~ 2018년 2월)\",\n            \"프로젝트 매니저로 신규 서비스 기획 및 개발 담당\"\n        ]\n        \n        for text in experience_texts:\n            self.assertTrue(self.recognizer.is_experience_entry(text))\n```\n\n4. Collect and analyze at least 20 diverse Korean resume samples from different industries and formats to identify common patterns and variations.\n\n5. Create a dataset of Korean resume sections in `data/korean_resume_sections/` with labeled examples for each section type.\n\n6. Implement a rule evaluation script in `scripts/evaluate_korean_rules.py` to measure the accuracy of the section recognition rules:\n```python\nimport os\nimport json\nimport re\nfrom src.parser.korean_section_recognizer import KoreanSectionRecognizer\n\ndef evaluate_rules(test_data_path, output_path):\n    \"\"\"Evaluate the Korean section recognition rules on test data.\"\"\"\n    recognizer = KoreanSectionRecognizer()\n    results = {\n        'total_sections': 0,\n        'correctly_identified': 0,\n        'section_results': {}\n    }\n    \n    # Load test data\n    with open(test_data_path, 'r', encoding='utf-8') as f:\n        test_data = json.load(f)\n    \n    # Evaluate each section type\n    for section_type, examples in test_data.items():\n        correct = 0\n        results['section_results'][section_type] = {'total': len(examples), 'correct': 0}\n        \n        for example in examples:\n            identified_section = recognizer.identify_section_by_heading(example['heading'])\n            if identified_section == section_type:\n                correct += 1\n                results['correctly_identified'] += 1\n            \n            results['total_sections'] += 1\n        \n        results['section_results'][section_type]['correct'] = correct\n        results['section_results'][section_type]['accuracy'] = correct / len(examples) if examples else 0\n    \n    # Calculate overall accuracy\n    results['overall_accuracy'] = results['correctly_identified'] / results['total_sections'] if results['total_sections'] > 0 else 0\n    \n    # Save results\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, ensure_ascii=False, indent=2)\n    \n    return results\n\nif __name__ == \"__main__\":\n    results = evaluate_rules('data/korean_resume_sections/test_data.json', 'data/korean_resume_sections/evaluation_results.json')\n    print(f\"Overall accuracy: {results['overall_accuracy']:.2%}\")\n    print(\"Section-specific results:\")\n    for section, res in results['section_results'].items():\n        print(f\"  {section}: {res['accuracy']:.2%} ({res['correct']}/{res['total']})\")\n```\n\n7. Refine the rules iteratively based on evaluation results, aiming for at least 90% accuracy across all section types.\n\n8. Document the Korean-specific patterns and rules in a separate markdown file at `docs/korean_resume_parsing.md`.",
        "testStrategy": "1. Create a comprehensive test suite with the following components:\n\n   a. Unit tests for the KoreanSectionRecognizer class:\n   - Test each method individually with various inputs\n   - Verify correct identification of all section types\n   - Test edge cases like mixed Korean/English content\n   - Test with different formatting styles (bullets, numbering, etc.)\n\n   b. Integration tests with the main parser:\n   - Test end-to-end parsing of Korean resumes\n   - Verify correct section identification and data extraction\n   - Test with resumes from different industries and formats\n\n2. Prepare a diverse dataset of Korean resume samples:\n   - Collect at least 20 different Korean resume formats\n   - Include resumes from various industries (IT, finance, healthcare, etc.)\n   - Include both traditional and modern resume styles\n   - Manually label each section in these resumes for ground truth comparison\n\n3. Implement automated evaluation:\n   - Run the evaluation script on the test dataset\n   - Calculate precision, recall, and F1 score for each section type\n   - Generate confusion matrix to identify common misclassifications\n   - Set a minimum threshold of 90% accuracy for each section type\n\n4. Perform cross-validation:\n   - Split the dataset into training and testing sets\n   - Use training set to refine rules\n   - Validate on the testing set\n   - Rotate and repeat to ensure robustness\n\n5. Manual review:\n   - Have Korean language experts review the results\n   - Identify patterns that the automated rules miss\n   - Document edge cases and special considerations\n\n6. Performance testing:\n   - Measure processing time for Korean resumes\n   - Compare with English resume processing time\n   - Optimize if necessary for production use\n\n7. Document test results:\n   - Create detailed reports of accuracy by section type\n   - Document any limitations or known issues\n   - Provide recommendations for future improvements",
        "status": "pending",
        "dependencies": [
          6,
          8,
          10
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Korean NLP Entity Extraction for Resumes",
        "description": "Develop functionality to extract named entities such as person names, organizations, dates, and job titles from Korean resumes using KoNLPy, KoSpaCy, and other Korean NLP libraries.",
        "details": "1. Create a new module `src/parser/korean_entity_extractor.py`:\n```python\nimport re\nfrom konlpy.tag import Mecab, Kkma\nimport spacy\nfrom typing import Dict, List, Any, Optional\n\nclass KoreanEntityExtractor:\n    \"\"\"Extract named entities from Korean resume text.\"\"\"\n    \n    def __init__(self, use_konlpy=True, use_kospacy=True):\n        self.use_konlpy = use_konlpy\n        self.use_kospacy = use_kospacy\n        \n        # Initialize NLP tools\n        if self.use_konlpy:\n            try:\n                self.mecab = Mecab()  # For morphological analysis\n                self.kkma = Kkma()    # Alternative tagger\n            except:\n                print(\"Warning: Could not initialize Mecab/Kkma. Check if KoNLPy is properly installed.\")\n                self.use_konlpy = False\n                \n        if self.use_kospacy:\n            try:\n                self.nlp = spacy.load(\"ko_core_news_lg\")  # Load Korean SpaCy model\n            except:\n                print(\"Warning: Could not load KoSpaCy model. Check if it's properly installed.\")\n                self.use_kospacy = False\n    \n    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n        \"\"\"\n        Extract named entities from the given text.\n        \n        Args:\n            text: Korean text from a resume section\n            \n        Returns:\n            Dictionary with entity types as keys and lists of extracted entities as values\n        \"\"\"\n        entities = {\n            \"PERSON\": [],      # Person names\n            \"ORG\": [],         # Organizations\n            \"DATE\": [],        # Dates\n            \"POSITION\": [],    # Job titles/positions\n            \"LOC\": [],         # Locations\n            \"EDUCATION\": [],   # Educational qualifications\n            \"SKILL\": []        # Skills/technologies\n        }\n        \n        # Use KoSpaCy for NER\n        if self.use_kospacy:\n            doc = self.nlp(text)\n            for ent in doc.ents:\n                if ent.label_ in entities:\n                    entities[ent.label_].append(ent.text)\n                    \n        # Use KoNLPy for additional entity extraction\n        if self.use_konlpy:\n            # Extract potential person names using Mecab\n            mecab_pos = self.mecab.pos(text)\n            self._extract_person_names(mecab_pos, entities)\n            \n            # Extract organizations and positions using patterns\n            self._extract_organizations(text, entities)\n            self._extract_positions(text, entities)\n            \n            # Extract dates using regex patterns\n            self._extract_dates(text, entities)\n        \n        # Remove duplicates and clean up entities\n        for entity_type in entities:\n            entities[entity_type] = list(set(entities[entity_type]))\n            \n        return entities\n    \n    def _extract_person_names(self, pos_tagged: List[tuple], entities: Dict[str, List[str]]):\n        \"\"\"Extract person names using POS patterns.\"\"\"\n        # Korean names typically consist of a family name followed by given name\n        # Look for sequences of NNP (Proper Noun) tags\n        i = 0\n        while i < len(pos_tagged) - 1:\n            if pos_tagged[i][1].startswith('NNP'):\n                name_parts = [pos_tagged[i][0]]\n                j = i + 1\n                while j < len(pos_tagged) and pos_tagged[j][1].startswith('NNP'):\n                    name_parts.append(pos_tagged[j][0])\n                    j += 1\n                \n                if len(name_parts) >= 2 and len(name_parts) <= 4:  # Korean names typically 2-4 characters\n                    full_name = ''.join(name_parts)\n                    if 2 <= len(full_name) <= 5:  # Typical Korean name length\n                        entities[\"PERSON\"].append(full_name)\n                i = j\n            else:\n                i += 1\n    \n    def _extract_organizations(self, text: str, entities: Dict[str, List[str]]):\n        \"\"\"Extract organization names using patterns and keywords.\"\"\"\n        # Common organization suffixes in Korean\n        org_patterns = [\n            r'[가-힣\\w]+(주식회사|㈜|(주)|회사|기업|그룹|학교|대학교|대학|연구소|재단|협회)',\n            r'[A-Za-z\\d]+ (Inc\\.|Corporation|Corp\\.|Co\\.|Ltd\\.)'\n        ]\n        \n        for pattern in org_patterns:\n            matches = re.finditer(pattern, text)\n            for match in matches:\n                entities[\"ORG\"].append(match.group(0))\n    \n    def _extract_positions(self, text: str, entities: Dict[str, List[str]]):\n        \"\"\"Extract job positions and titles.\"\"\"\n        # Common job title suffixes in Korean\n        position_patterns = [\n            r'[가-힣\\w]+(사원|주임|대리|과장|차장|부장|이사|상무|전무|사장|대표|팀장|리더|매니저|엔지니어|개발자|연구원)',\n            r'[A-Za-z\\d]+ (Engineer|Developer|Manager|Director|Lead|Officer|Researcher)'\n        ]\n        \n        for pattern in position_patterns:\n            matches = re.finditer(pattern, text)\n            for match in matches:\n                entities[\"POSITION\"].append(match.group(0))\n    \n    def _extract_dates(self, text: str, entities: Dict[str, List[str]]):\n        \"\"\"Extract dates using regex patterns.\"\"\"\n        # Korean and standard date formats\n        date_patterns = [\n            r'\\d{4}년\\s*\\d{1,2}월\\s*\\d{1,2}일',  # YYYY년 MM월 DD일\n            r'\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2}',    # YYYY.MM.DD\n            r'\\d{4}-\\d{1,2}-\\d{1,2}',            # YYYY-MM-DD\n            r'\\d{4}년\\s*\\d{1,2}월',              # YYYY년 MM월\n            r'\\d{4}\\.\\s*\\d{1,2}',                # YYYY.MM\n            r'\\d{4}-\\d{1,2}'                     # YYYY-MM\n        ]\n        \n        for pattern in date_patterns:\n            matches = re.finditer(pattern, text)\n            for match in matches:\n                entities[\"DATE\"].append(match.group(0))\n```\n\n2. Integrate the entity extractor with the Korean section recognizer in `src/parser/korean_resume_parser.py`:\n```python\nfrom .korean_section_recognizer import KoreanSectionRecognizer\nfrom .korean_entity_extractor import KoreanEntityExtractor\n\nclass KoreanResumeParser:\n    \"\"\"Parser for Korean resumes with entity extraction capabilities.\"\"\"\n    \n    def __init__(self):\n        self.section_recognizer = KoreanSectionRecognizer()\n        self.entity_extractor = KoreanEntityExtractor()\n    \n    def parse(self, document_text):\n        \"\"\"\n        Parse a Korean resume document and extract structured information.\n        \n        Args:\n            document_text: Full text content of the resume\n            \n        Returns:\n            Dictionary with parsed sections and extracted entities\n        \"\"\"\n        # First identify sections\n        sections = self.section_recognizer.identify_sections(document_text)\n        \n        # Extract entities from each section\n        parsed_resume = {}\n        for section_name, section_text in sections.items():\n            parsed_resume[section_name] = {\n                'text': section_text,\n                'entities': self.entity_extractor.extract_entities(section_text)\n            }\n            \n        return parsed_resume\n```\n\n3. Update the JSON converter to include the extracted entities in `src/converter/json_converter.py`:\n```python\ndef convert_entities_to_json(entities_dict):\n    \"\"\"Convert extracted entities to a standardized JSON format.\"\"\"\n    result = {}\n    \n    # Map entity types to standardized fields\n    if 'PERSON' in entities_dict and entities_dict['PERSON']:\n        result['name'] = entities_dict['PERSON'][0]  # Use first detected name\n        \n    if 'ORG' in entities_dict:\n        result['organizations'] = entities_dict['ORG']\n        \n    if 'DATE' in entities_dict:\n        result['dates'] = entities_dict['DATE']\n        \n    if 'POSITION' in entities_dict:\n        result['positions'] = entities_dict['POSITION']\n        \n    if 'LOC' in entities_dict:\n        result['locations'] = entities_dict['LOC']\n        \n    if 'EDUCATION' in entities_dict:\n        result['education'] = entities_dict['EDUCATION']\n        \n    if 'SKILL' in entities_dict:\n        result['skills'] = entities_dict['SKILL']\n        \n    return result\n```\n\n4. Install required Korean NLP libraries in the project:\n   - Add the following to requirements.txt:\n   ```\n   konlpy>=0.6.0\n   spacy>=3.5.0\n   ko-core-news-lg @ https://github.com/explosion/spacy-models/releases/download/ko_core_news_lg-3.5.0/ko_core_news_lg-3.5.0-py3-none-any.whl\n   ```\n\n5. Create a utility script to test entity extraction independently in `scripts/test_entity_extraction.py`:\n```python\nimport sys\nimport os\nimport json\n\n# Add the src directory to the Python path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom src.parser.korean_entity_extractor import KoreanEntityExtractor\n\ndef main():\n    if len(sys.argv) < 2:\n        print(\"Usage: python test_entity_extraction.py <text_file>\")\n        return\n        \n    file_path = sys.argv[1]\n    \n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n            \n        extractor = KoreanEntityExtractor()\n        entities = extractor.extract_entities(text)\n        \n        print(json.dumps(entities, ensure_ascii=False, indent=2))\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n6. Handle potential installation issues with Korean NLP libraries:\n   - Create a detailed installation guide for Korean NLP dependencies in `docs/korean_nlp_setup.md`\n   - Include instructions for installing MeCab, KoNLPy, and KoSpaCy with proper system dependencies\n   - Document common issues and solutions for different operating systems",
        "testStrategy": "1. Create unit tests for the KoreanEntityExtractor class in `tests/parser/test_korean_entity_extractor.py`:\n```python\nimport unittest\nfrom src.parser.korean_entity_extractor import KoreanEntityExtractor\n\nclass TestKoreanEntityExtractor(unittest.TestCase):\n    def setUp(self):\n        self.extractor = KoreanEntityExtractor()\n        \n    def test_person_name_extraction(self):\n        # Test with common Korean name patterns\n        text = \"홍길동은 한국대학교를 졸업했습니다. 김철수와 박영희도 같은 학교 출신입니다.\"\n        entities = self.extractor.extract_entities(text)\n        \n        self.assertIn(\"홍길동\", entities[\"PERSON\"])\n        self.assertIn(\"김철수\", entities[\"PERSON\"])\n        self.assertIn(\"박영희\", entities[\"PERSON\"])\n        \n    def test_organization_extraction(self):\n        # Test with various organization formats\n        text = \"삼성전자(주)에서 근무했으며, 이후 LG전자 주식회사와 현대자동차그룹에서 일했습니다.\"\n        entities = self.extractor.extract_entities(text)\n        \n        self.assertIn(\"삼성전자(주)\", entities[\"ORG\"])\n        self.assertIn(\"LG전자 주식회사\", entities[\"ORG\"])\n        self.assertIn(\"현대자동차그룹\", entities[\"ORG\"])\n        \n    def test_date_extraction(self):\n        # Test with various date formats\n        text = \"2018년 3월부터 2020년 5월까지 근무. 2021.06.01에 입사하여 2022-12-31에 퇴사했습니다.\"\n        entities = self.extractor.extract_entities(text)\n        \n        self.assertIn(\"2018년 3월\", entities[\"DATE\"])\n        self.assertIn(\"2020년 5월\", entities[\"DATE\"])\n        self.assertIn(\"2021.06.01\", entities[\"DATE\"])\n        self.assertIn(\"2022-12-31\", entities[\"DATE\"])\n        \n    def test_position_extraction(self):\n        # Test with various job titles\n        text = \"소프트웨어 엔지니어로 시작하여 프로젝트 매니저, 이후 개발팀장을 거쳐 CTO로 승진했습니다.\"\n        entities = self.extractor.extract_entities(text)\n        \n        self.assertIn(\"소프트웨어 엔지니어\", entities[\"POSITION\"])\n        self.assertIn(\"프로젝트 매니저\", entities[\"POSITION\"])\n        self.assertIn(\"개발팀장\", entities[\"POSITION\"])\n        \n    def test_mixed_language_content(self):\n        # Test with mixed Korean and English content\n        text = \"John Kim은 Google Korea에서 Senior Software Engineer로 근무했습니다.\"\n        entities = self.extractor.extract_entities(text)\n        \n        self.assertIn(\"John Kim\", entities[\"PERSON\"])\n        self.assertIn(\"Google Korea\", entities[\"ORG\"])\n        self.assertIn(\"Senior Software Engineer\", entities[\"POSITION\"])\n```\n\n2. Create integration tests to verify entity extraction works with the section recognizer:\n```python\nimport unittest\nfrom src.parser.korean_resume_parser import KoreanResumeParser\n\nclass TestKoreanResumeParser(unittest.TestCase):\n    def setUp(self):\n        self.parser = KoreanResumeParser()\n        \n    def test_full_resume_parsing(self):\n        # Test with a sample resume text\n        resume_text = \"\"\"\n        # 인적사항\n        이름: 홍길동\n        이메일: hong@example.com\n        전화번호: 010-1234-5678\n        \n        # 학력사항\n        2010년 3월 - 2014년 2월: 서울대학교 컴퓨터공학과 학사\n        2014년 3월 - 2016년 2월: 한국과학기술원 인공지능학과 석사\n        \n        # 경력사항\n        2016년 3월 - 2019년 5월: 삼성전자(주) 소프트웨어 엔지니어\n        2019년 6월 - 현재: 네이버 주식회사 선임연구원\n        \"\"\"\n        \n        result = self.parser.parse(resume_text)\n        \n        # Check if sections were correctly identified\n        self.assertIn(\"personal_info\", result)\n        self.assertIn(\"education\", result)\n        self.assertIn(\"experience\", result)\n        \n        # Check if entities were correctly extracted\n        self.assertIn(\"홍길동\", result[\"personal_info\"][\"entities\"][\"PERSON\"])\n        self.assertIn(\"서울대학교\", result[\"education\"][\"entities\"][\"ORG\"])\n        self.assertIn(\"한국과학기술원\", result[\"education\"][\"entities\"][\"ORG\"])\n        self.assertIn(\"삼성전자(주)\", result[\"experience\"][\"entities\"][\"ORG\"])\n        self.assertIn(\"네이버 주식회사\", result[\"experience\"][\"entities\"][\"ORG\"])\n        self.assertIn(\"소프트웨어 엔지니어\", result[\"experience\"][\"entities\"][\"POSITION\"])\n        self.assertIn(\"선임연구원\", result[\"experience\"][\"entities\"][\"POSITION\"])\n```\n\n3. Create a test script to evaluate entity extraction accuracy on a set of sample resumes:\n```python\nimport os\nimport json\nimport csv\nfrom src.parser.korean_entity_extractor import KoreanEntityExtractor\n\ndef evaluate_entity_extraction():\n    \"\"\"Evaluate entity extraction accuracy on sample resumes.\"\"\"\n    # Directory containing sample resume texts\n    samples_dir = \"tests/samples/korean_resumes\"\n    \n    # Output CSV for evaluation results\n    results_file = \"tests/results/entity_extraction_evaluation.csv\"\n    \n    extractor = KoreanEntityExtractor()\n    \n    results = []\n    \n    for filename in os.listdir(samples_dir):\n        if filename.endswith(\".txt\"):\n            file_path = os.path.join(samples_dir, filename)\n            \n            with open(file_path, 'r', encoding='utf-8') as f:\n                text = f.read()\n                \n            # Extract entities\n            entities = extractor.extract_entities(text)\n            \n            # Count entities by type\n            entity_counts = {entity_type: len(entities[entity_type]) for entity_type in entities}\n            \n            # Add to results\n            results.append({\n                'filename': filename,\n                'total_entities': sum(entity_counts.values()),\n                **entity_counts\n            })\n    \n    # Write results to CSV\n    with open(results_file, 'w', newline='', encoding='utf-8') as f:\n        fieldnames = ['filename', 'total_entities', 'PERSON', 'ORG', 'DATE', 'POSITION', 'LOC', 'EDUCATION', 'SKILL']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for result in results:\n            writer.writerow(result)\n            \n    print(f\"Evaluation results written to {results_file}\")\n\nif __name__ == \"__main__\":\n    evaluate_entity_extraction()\n```\n\n4. Manual testing with real Korean resumes:\n   - Collect a diverse set of Korean resume samples (at least 10)\n   - Run the entity extractor on each sample and manually verify the accuracy of extracted entities\n   - Document any patterns or formats that cause issues\n   - Create a spreadsheet tracking precision and recall for each entity type\n\n5. Performance testing:\n   - Measure processing time for different resume sizes\n   - Compare performance between KoNLPy and KoSpaCy approaches\n   - Identify any bottlenecks in the entity extraction process",
        "status": "pending",
        "dependencies": [
          6,
          8,
          11
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Implement Document Layout Analysis with HuggingFace Models for Korean Resumes",
        "description": "Experiment with and implement document layout analysis using HuggingFace models like Donut and LayoutLM to automatically extract sections and fields from various Korean resume layouts.",
        "details": "1. Set up a new module `src/parser/layout_analyzer.py` for document layout analysis:\n```python\nimport torch\nfrom transformers import (\n    DonutProcessor, \n    VisionEncoderDecoderModel,\n    LayoutLMv2Processor, \n    LayoutLMv2ForTokenClassification\n)\nfrom PIL import Image\nimport numpy as np\nfrom typing import Dict, List, Any, Optional\n\nclass DocumentLayoutAnalyzer:\n    \"\"\"Analyze document layouts using transformer-based models.\"\"\"\n    \n    def __init__(self, model_type=\"layoutlm\", model_name=None):\n        \"\"\"\n        Initialize the document layout analyzer.\n        \n        Args:\n            model_type: Type of model to use ('layoutlm', 'donut', etc.)\n            model_name: Specific model name/path to load\n        \"\"\"\n        self.model_type = model_type\n        self.model_name = model_name or self._get_default_model_name()\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._load_model()\n    \n    def _get_default_model_name(self):\n        \"\"\"Get default model name based on model type.\"\"\"\n        if self.model_type == \"layoutlm\":\n            return \"microsoft/layoutlmv2-base-uncased\"\n        elif self.model_type == \"donut\":\n            return \"naver-clova-ix/donut-base\"\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n    \n    def _load_model(self):\n        \"\"\"Load the appropriate model based on model_type.\"\"\"\n        if self.model_type == \"layoutlm\":\n            self.processor = LayoutLMv2Processor.from_pretrained(self.model_name)\n            self.model = LayoutLMv2ForTokenClassification.from_pretrained(self.model_name).to(self.device)\n        elif self.model_type == \"donut\":\n            self.processor = DonutProcessor.from_pretrained(self.model_name)\n            self.model = VisionEncoderDecoderModel.from_pretrained(self.model_name).to(self.device)\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n    \n    def analyze_document(self, image_path):\n        \"\"\"\n        Analyze document layout from an image.\n        \n        Args:\n            image_path: Path to the document image\n            \n        Returns:\n            Dictionary containing extracted layout information\n        \"\"\"\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        if self.model_type == \"layoutlm\":\n            return self._analyze_with_layoutlm(image)\n        elif self.model_type == \"donut\":\n            return self._analyze_with_donut(image)\n    \n    def _analyze_with_layoutlm(self, image):\n        \"\"\"Process image with LayoutLM model.\"\"\"\n        # Implement LayoutLM-specific processing\n        # This would include OCR, encoding, and inference\n        pass\n    \n    def _analyze_with_donut(self, image):\n        \"\"\"Process image with Donut model.\"\"\"\n        # Implement Donut-specific processing\n        pass\n```\n\n2. Create a specialized Korean resume layout analyzer in `src/parser/korean_layout_analyzer.py`:\n```python\nfrom .layout_analyzer import DocumentLayoutAnalyzer\nimport re\nfrom konlpy.tag import Mecab\nfrom typing import Dict, List, Any\n\nclass KoreanResumeLayoutAnalyzer(DocumentLayoutAnalyzer):\n    \"\"\"Specialized layout analyzer for Korean resumes.\"\"\"\n    \n    def __init__(self, model_type=\"layoutlm\", model_name=None):\n        super().__init__(model_type, model_name)\n        self.mecab = Mecab()\n        \n        # Korean section keywords\n        self.section_keywords = {\n            'personal_info': ['개인정보', '인적사항', '신상정보', '기본정보'],\n            'education': ['학력', '학력사항', '교육', '학교', '교육이력'],\n            'experience': ['경력', '경력사항', '직장경력', '업무경험', '프로젝트'],\n            'skills': ['기술', '기술스택', '보유기술', '자격증', '스킬'],\n            'languages': ['언어', '외국어', '어학능력', '어학점수'],\n            'additional': ['수상', '봉사활동', '취미', '특기', '기타']\n        }\n    \n    def analyze_korean_resume(self, image_path):\n        \"\"\"\n        Analyze Korean resume layout and extract structured information.\n        \n        Args:\n            image_path: Path to the resume image\n            \n        Returns:\n            Dictionary with structured resume information\n        \"\"\"\n        # Get base layout analysis\n        layout_data = self.analyze_document(image_path)\n        \n        # Enhance with Korean-specific processing\n        enhanced_data = self._enhance_with_korean_nlp(layout_data)\n        \n        # Structure the data into resume sections\n        structured_resume = self._structure_resume_data(enhanced_data)\n        \n        return structured_resume\n    \n    def _enhance_with_korean_nlp(self, layout_data):\n        \"\"\"Enhance layout data with Korean NLP processing.\"\"\"\n        # Implement Korean-specific NLP enhancements\n        pass\n    \n    def _structure_resume_data(self, enhanced_data):\n        \"\"\"Structure the enhanced data into resume sections.\"\"\"\n        # Implement section structuring logic\n        pass\n    \n    def _identify_section_type(self, text):\n        \"\"\"Identify the type of section based on text content.\"\"\"\n        # Implement section identification logic\n        pass\n```\n\n3. Create a fine-tuning script for adapting pre-trained models to Korean resumes in `src/training/finetune_layout_model.py`:\n```python\nimport torch\nfrom transformers import (\n    Trainer, \n    TrainingArguments,\n    LayoutLMv2ForTokenClassification,\n    LayoutLMv2Processor\n)\nfrom datasets import Dataset\nimport os\nimport json\nfrom PIL import Image\n\ndef prepare_dataset(data_dir, processor):\n    \"\"\"Prepare dataset for fine-tuning.\"\"\"\n    # Implementation for dataset preparation\n    pass\n\ndef train_layout_model(\n    model_name=\"microsoft/layoutlmv2-base-uncased\",\n    data_dir=\"data/korean_resumes\",\n    output_dir=\"models/korean_resume_layoutlm\",\n    num_epochs=5,\n    batch_size=8\n):\n    \"\"\"\n    Fine-tune a layout model on Korean resume data.\n    \n    Args:\n        model_name: Base model to fine-tune\n        data_dir: Directory containing training data\n        output_dir: Directory to save the fine-tuned model\n        num_epochs: Number of training epochs\n        batch_size: Training batch size\n    \"\"\"\n    # Load model and processor\n    processor = LayoutLMv2Processor.from_pretrained(model_name)\n    model = LayoutLMv2ForTokenClassification.from_pretrained(model_name)\n    \n    # Prepare dataset\n    train_dataset = prepare_dataset(os.path.join(data_dir, \"train\"), processor)\n    eval_dataset = prepare_dataset(os.path.join(data_dir, \"eval\"), processor)\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=os.path.join(output_dir, \"logs\"),\n        logging_steps=10,\n        learning_rate=5e-5,\n        load_best_model_at_end=True,\n    )\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n    )\n    \n    # Train model\n    trainer.train()\n    \n    # Save the fine-tuned model\n    model.save_pretrained(output_dir)\n    processor.save_pretrained(output_dir)\n```\n\n4. Integrate the layout analyzer with the existing resume parser in `src/parser/resume_parser.py`:\n```python\n# Add imports\nfrom .korean_layout_analyzer import KoreanResumeLayoutAnalyzer\n\nclass ResumeParser:\n    # Add to __init__\n    def __init__(self, use_layout_analysis=False, layout_model_type=\"layoutlm\"):\n        # Existing initialization code\n        self.use_layout_analysis = use_layout_analysis\n        if self.use_layout_analysis:\n            self.layout_analyzer = KoreanResumeLayoutAnalyzer(model_type=layout_model_type)\n    \n    # Add new method\n    def parse_with_layout_analysis(self, file_path):\n        \"\"\"\n        Parse resume using document layout analysis.\n        \n        Args:\n            file_path: Path to the resume file (image or PDF)\n            \n        Returns:\n            Structured resume data\n        \"\"\"\n        # Convert document to image if needed\n        image_path = self._convert_to_image(file_path)\n        \n        # Analyze layout\n        resume_data = self.layout_analyzer.analyze_korean_resume(image_path)\n        \n        # Post-process and validate\n        processed_data = self._post_process_resume_data(resume_data)\n        \n        return processed_data\n    \n    def _convert_to_image(self, file_path):\n        \"\"\"Convert document to image if it's not already an image.\"\"\"\n        # Implementation for document-to-image conversion\n        pass\n```\n\n5. Create a utility for comparing results between rule-based and layout-based approaches in `src/utils/parser_comparison.py`:\n```python\nimport json\nimport pandas as pd\nfrom ..parser.resume_parser import ResumeParser\n\ndef compare_parsing_approaches(test_files, ground_truth_dir=None):\n    \"\"\"\n    Compare different parsing approaches on test files.\n    \n    Args:\n        test_files: List of test file paths\n        ground_truth_dir: Directory containing ground truth JSON files\n        \n    Returns:\n        DataFrame with comparison metrics\n    \"\"\"\n    # Initialize parsers\n    rule_parser = ResumeParser(use_layout_analysis=False)\n    layoutlm_parser = ResumeParser(use_layout_analysis=True, layout_model_type=\"layoutlm\")\n    donut_parser = ResumeParser(use_layout_analysis=True, layout_model_type=\"donut\")\n    \n    results = []\n    \n    for file_path in test_files:\n        file_name = os.path.basename(file_path)\n        \n        # Parse with different approaches\n        rule_result = rule_parser.parse(file_path)\n        layoutlm_result = layoutlm_parser.parse_with_layout_analysis(file_path)\n        donut_result = donut_parser.parse_with_layout_analysis(file_path)\n        \n        # Load ground truth if available\n        ground_truth = None\n        if ground_truth_dir:\n            gt_path = os.path.join(ground_truth_dir, file_name.replace(os.path.splitext(file_name)[1], '.json'))\n            if os.path.exists(gt_path):\n                with open(gt_path, 'r', encoding='utf-8') as f:\n                    ground_truth = json.load(f)\n        \n        # Calculate metrics\n        metrics = calculate_metrics(\n            rule_result, \n            layoutlm_result, \n            donut_result, \n            ground_truth\n        )\n        \n        results.append({\n            'file_name': file_name,\n            **metrics\n        })\n    \n    return pd.DataFrame(results)\n\ndef calculate_metrics(rule_result, layoutlm_result, donut_result, ground_truth=None):\n    \"\"\"Calculate comparison metrics between different parsing results.\"\"\"\n    # Implementation for metrics calculation\n    pass\n```\n\n6. Update requirements.txt to include necessary dependencies:\n```\n# Add these to existing requirements\ntransformers>=4.20.0\ntorch>=1.10.0\nPillow>=8.0.0\npdf2image>=1.16.0\ndatasets>=2.0.0\n```",
        "testStrategy": "1. Create unit tests for the DocumentLayoutAnalyzer class in `tests/parser/test_layout_analyzer.py`:\n```python\nimport unittest\nimport os\nfrom PIL import Image\nimport torch\nfrom src.parser.layout_analyzer import DocumentLayoutAnalyzer\n\nclass TestDocumentLayoutAnalyzer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # Skip tests if GPU not available and model requires it\n        cls.skip_gpu_tests = not torch.cuda.is_available()\n        \n        # Create test directory if it doesn't exist\n        os.makedirs('tests/test_data/images', exist_ok=True)\n        \n        # Create a simple test image if it doesn't exist\n        cls.test_image_path = 'tests/test_data/images/test_resume.png'\n        if not os.path.exists(cls.test_image_path):\n            img = Image.new('RGB', (1000, 1500), color='white')\n            img.save(cls.test_image_path)\n    \n    def test_init_layoutlm(self):\n        \"\"\"Test initialization with LayoutLM model.\"\"\"\n        analyzer = DocumentLayoutAnalyzer(model_type=\"layoutlm\")\n        self.assertEqual(analyzer.model_type, \"layoutlm\")\n        self.assertTrue(hasattr(analyzer, 'processor'))\n        self.assertTrue(hasattr(analyzer, 'model'))\n    \n    def test_init_donut(self):\n        \"\"\"Test initialization with Donut model.\"\"\"\n        analyzer = DocumentLayoutAnalyzer(model_type=\"donut\")\n        self.assertEqual(analyzer.model_type, \"donut\")\n        self.assertTrue(hasattr(analyzer, 'processor'))\n        self.assertTrue(hasattr(analyzer, 'model'))\n    \n    def test_invalid_model_type(self):\n        \"\"\"Test initialization with invalid model type.\"\"\"\n        with self.assertRaises(ValueError):\n            DocumentLayoutAnalyzer(model_type=\"invalid_model\")\n    \n    @unittest.skipIf(True, \"Skipping test that requires model download\")\n    def test_analyze_document(self):\n        \"\"\"Test document analysis functionality.\"\"\"\n        analyzer = DocumentLayoutAnalyzer(model_type=\"layoutlm\")\n        result = analyzer.analyze_document(self.test_image_path)\n        self.assertIsNotNone(result)\n```\n\n2. Create unit tests for the KoreanResumeLayoutAnalyzer class in `tests/parser/test_korean_layout_analyzer.py`:\n```python\nimport unittest\nimport os\nfrom PIL import Image\nimport torch\nfrom src.parser.korean_layout_analyzer import KoreanResumeLayoutAnalyzer\n\nclass TestKoreanResumeLayoutAnalyzer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # Skip tests if GPU not available and model requires it\n        cls.skip_gpu_tests = not torch.cuda.is_available()\n        \n        # Create test directory if it doesn't exist\n        os.makedirs('tests/test_data/korean_resumes', exist_ok=True)\n        \n        # Create a simple test image if it doesn't exist\n        cls.test_image_path = 'tests/test_data/korean_resumes/test_korean_resume.png'\n        if not os.path.exists(cls.test_image_path):\n            img = Image.new('RGB', (1000, 1500), color='white')\n            img.save(cls.test_image_path)\n    \n    def test_init(self):\n        \"\"\"Test initialization of Korean resume layout analyzer.\"\"\"\n        analyzer = KoreanResumeLayoutAnalyzer()\n        self.assertTrue(hasattr(analyzer, 'mecab'))\n        self.assertTrue(hasattr(analyzer, 'section_keywords'))\n    \n    @unittest.skipIf(True, \"Skipping test that requires model download\")\n    def test_analyze_korean_resume(self):\n        \"\"\"Test Korean resume analysis functionality.\"\"\"\n        analyzer = KoreanResumeLayoutAnalyzer()\n        result = analyzer.analyze_korean_resume(self.test_image_path)\n        self.assertIsNotNone(result)\n    \n    def test_section_keywords(self):\n        \"\"\"Test that section keywords are properly defined.\"\"\"\n        analyzer = KoreanResumeLayoutAnalyzer()\n        self.assertIn('personal_info', analyzer.section_keywords)\n        self.assertIn('education', analyzer.section_keywords)\n        self.assertIn('experience', analyzer.section_keywords)\n        self.assertIn('skills', analyzer.section_keywords)\n```\n\n3. Create integration tests in `tests/integration/test_layout_parsing.py`:\n```python\nimport unittest\nimport os\nimport json\nfrom src.parser.resume_parser import ResumeParser\n\nclass TestLayoutParsing(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # Set up test data paths\n        cls.test_data_dir = 'tests/test_data/korean_resumes'\n        cls.sample_resume = os.path.join(cls.test_data_dir, 'sample_korean_resume.pdf')\n        \n        # Skip if test data doesn't exist\n        cls.skip_tests = not os.path.exists(cls.sample_resume)\n    \n    @unittest.skipIf(True, \"Skipping test that requires model download\")\n    def test_rule_vs_layout_parsing(self):\n        \"\"\"Test and compare rule-based vs. layout-based parsing.\"\"\"\n        if self.skip_tests:\n            self.skipTest(\"Test data not available\")\n        \n        # Initialize parsers\n        rule_parser = ResumeParser(use_layout_analysis=False)\n        layout_parser = ResumeParser(use_layout_analysis=True)\n        \n        # Parse with both approaches\n        rule_result = rule_parser.parse(self.sample_resume)\n        layout_result = layout_parser.parse_with_layout_analysis(self.sample_resume)\n        \n        # Basic validation of results\n        self.assertIsNotNone(rule_result)\n        self.assertIsNotNone(layout_result)\n        \n        # Check that both results have expected sections\n        for result in [rule_result, layout_result]:\n            self.assertIn('personal_info', result)\n            self.assertIn('education', result)\n            self.assertIn('experience', result)\n```\n\n4. Create a benchmark script in `scripts/benchmark_layout_models.py`:\n```python\nimport argparse\nimport os\nimport json\nimport time\nimport pandas as pd\nfrom src.parser.resume_parser import ResumeParser\nfrom src.utils.parser_comparison import compare_parsing_approaches\n\ndef main():\n    parser = argparse.ArgumentParser(description='Benchmark different resume parsing approaches')\n    parser.add_argument('--data-dir', type=str, required=True, help='Directory containing test resume files')\n    parser.add_argument('--ground-truth-dir', type=str, help='Directory containing ground truth JSON files')\n    parser.add_argument('--output-file', type=str, default='benchmark_results.csv', help='Output file for benchmark results')\n    args = parser.parse_args()\n    \n    # Get list of test files\n    test_files = []\n    for root, _, files in os.walk(args.data_dir):\n        for file in files:\n            if file.endswith(('.pdf', '.docx', '.png', '.jpg', '.jpeg')):\n                test_files.append(os.path.join(root, file))\n    \n    print(f\"Found {len(test_files)} test files\")\n    \n    # Run comparison\n    results = compare_parsing_approaches(test_files, args.ground_truth_dir)\n    \n    # Save results\n    results.to_csv(args.output_file, index=False)\n    print(f\"Results saved to {args.output_file}\")\n    \n    # Print summary\n    print(\"\\nSummary:\")\n    print(results.describe())\n\nif __name__ == \"__main__\":\n    main()\n```\n\n5. Manual testing:\n   - Test with a diverse set of Korean resume formats (different layouts, fonts, and structures)\n   - Compare the accuracy of section identification between rule-based and layout-based approaches\n   - Evaluate performance on resumes with complex layouts, tables, and graphics\n   - Test with both high-quality scans and lower-quality images to assess robustness\n\n6. Performance evaluation:\n   - Measure processing time for different models and approaches\n   - Evaluate memory usage during processing\n   - Assess accuracy of section identification and field extraction\n   - Compare results with ground truth data when available",
        "status": "pending",
        "dependencies": [
          6,
          8,
          11,
          12
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Collect and Analyze Korean Resume Samples for Parser Testing and Improvement",
        "description": "Collect diverse Korean resume samples and use them to iteratively test and improve the performance of the resume parser, recognition rules, and models.",
        "details": "1. Create a structured dataset of Korean resume samples:\n   - Collect at least 50 diverse Korean resume samples in various formats (DOCX, PDF)\n   - Categorize samples by layout style, content organization, and industry\n   - Ensure samples cover different experience levels (entry, mid, senior)\n   - Include both traditional and modern resume formats\n   - Store samples in `data/samples/korean_resumes/` with appropriate naming convention\n\n2. Develop a testing and evaluation framework in `src/evaluation/parser_evaluator.py`:\n```python\nimport os\nimport json\nimport pandas as pd\nfrom typing import Dict, List, Any\nfrom src.parser.docx_parser import DocxParser\nfrom src.parser.layout_analyzer import DocumentLayoutAnalyzer\nfrom src.parser.korean_entity_extractor import KoreanEntityExtractor\nfrom src.parser.korean_section_recognizer import KoreanSectionRecognizer\n\nclass ParserEvaluator:\n    \"\"\"Evaluate and benchmark parser performance on Korean resumes.\"\"\"\n    \n    def __init__(self, sample_dir, ground_truth_dir=None):\n        self.sample_dir = sample_dir\n        self.ground_truth_dir = ground_truth_dir\n        self.parser = DocxParser()\n        self.layout_analyzer = DocumentLayoutAnalyzer()\n        self.entity_extractor = KoreanEntityExtractor()\n        self.section_recognizer = KoreanSectionRecognizer()\n        \n    def create_ground_truth(self, output_dir):\n        \"\"\"Process samples and save results for manual correction.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        \n        for filename in os.listdir(self.sample_dir):\n            if filename.endswith('.docx'):\n                file_path = os.path.join(self.sample_dir, filename)\n                try:\n                    parsed_data = self.parser.parse(file_path)\n                    output_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}.json\")\n                    with open(output_path, 'w', encoding='utf-8') as f:\n                        json.dump(parsed_data, f, ensure_ascii=False, indent=2)\n                except Exception as e:\n                    print(f\"Error processing {filename}: {str(e)}\")\n    \n    def evaluate_parser(self):\n        \"\"\"Evaluate parser performance against ground truth data.\"\"\"\n        if not self.ground_truth_dir:\n            raise ValueError(\"Ground truth directory not specified\")\n            \n        results = []\n        \n        for filename in os.listdir(self.sample_dir):\n            if filename.endswith('.docx'):\n                file_path = os.path.join(self.sample_dir, filename)\n                gt_path = os.path.join(self.ground_truth_dir, f\"{os.path.splitext(filename)[0]}.json\")\n                \n                if not os.path.exists(gt_path):\n                    continue\n                    \n                try:\n                    # Parse the resume\n                    parsed_data = self.parser.parse(file_path)\n                    \n                    # Load ground truth\n                    with open(gt_path, 'r', encoding='utf-8') as f:\n                        ground_truth = json.load(f)\n                    \n                    # Compare and calculate metrics\n                    metrics = self._calculate_metrics(parsed_data, ground_truth)\n                    metrics['filename'] = filename\n                    results.append(metrics)\n                except Exception as e:\n                    print(f\"Error evaluating {filename}: {str(e)}\")\n        \n        # Aggregate results\n        return pd.DataFrame(results)\n    \n    def _calculate_metrics(self, parsed_data, ground_truth):\n        \"\"\"Calculate precision, recall, and F1 score for each section.\"\"\"\n        metrics = {}\n        \n        # Calculate section detection accuracy\n        parsed_sections = set(parsed_data.keys())\n        gt_sections = set(ground_truth.keys())\n        \n        metrics['section_precision'] = len(parsed_sections.intersection(gt_sections)) / len(parsed_sections) if parsed_sections else 0\n        metrics['section_recall'] = len(parsed_sections.intersection(gt_sections)) / len(gt_sections) if gt_sections else 0\n        \n        if metrics['section_precision'] + metrics['section_recall'] > 0:\n            metrics['section_f1'] = 2 * (metrics['section_precision'] * metrics['section_recall']) / (metrics['section_precision'] + metrics['section_recall'])\n        else:\n            metrics['section_f1'] = 0\n        \n        # Calculate entity extraction accuracy for each section\n        # This is a simplified version - real implementation would be more detailed\n        \n        return metrics\n    \n    def analyze_error_patterns(self):\n        \"\"\"Identify common error patterns in parsing results.\"\"\"\n        # Implementation for error pattern analysis\n        pass\n    \n    def generate_report(self, output_path):\n        \"\"\"Generate a comprehensive evaluation report.\"\"\"\n        results = self.evaluate_parser()\n        \n        # Generate summary statistics\n        summary = {\n            'overall_f1': results['section_f1'].mean(),\n            'sample_count': len(results),\n            'error_rate': (results['section_f1'] < 0.8).mean(),\n            'perfect_parse_rate': (results['section_f1'] == 1.0).mean()\n        }\n        \n        # Save report\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(\"# Korean Resume Parser Evaluation Report\\n\\n\")\n            f.write(f\"## Summary\\n\\n\")\n            for key, value in summary.items():\n                f.write(f\"- {key}: {value:.4f}\\n\")\n            \n            f.write(\"\\n## Detailed Results\\n\\n\")\n            f.write(results.to_markdown())\n```\n\n3. Create a manual annotation tool in `src/tools/resume_annotator.py` to help create ground truth data:\n```python\nimport os\nimport json\nimport tkinter as tk\nfrom tkinter import filedialog, messagebox\nimport pandas as pd\nfrom PIL import Image, ImageTk\nfrom src.parser.docx_parser import DocxParser\n\nclass ResumeAnnotator:\n    \"\"\"GUI tool for manually annotating Korean resumes.\"\"\"\n    \n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"Korean Resume Annotator\")\n        self.root.geometry(\"1200x800\")\n        \n        self.parser = DocxParser()\n        self.current_file = None\n        self.parsed_data = None\n        \n        self._setup_ui()\n    \n    def _setup_ui(self):\n        # Implementation of UI setup\n        pass\n    \n    def load_file(self):\n        # Implementation of file loading\n        pass\n    \n    def save_annotation(self):\n        # Implementation of annotation saving\n        pass\n```\n\n4. Implement an iterative improvement process:\n   - Run initial tests on all samples and identify common failure patterns\n   - Categorize errors (section recognition, entity extraction, layout analysis)\n   - Prioritize improvements based on frequency and impact\n   - Implement fixes in the respective modules\n   - Re-test to verify improvements\n   - Document patterns and solutions for future reference\n\n5. Create a benchmark dataset:\n   - Select 20 representative samples from the collection\n   - Create manually verified ground truth JSON files\n   - Use this dataset for regression testing when making changes\n\n6. Develop specific test cases for challenging scenarios:\n   - Resumes with unusual layouts\n   - Resumes with mixed Korean/English content\n   - Resumes with tables and complex formatting\n   - Resumes with uncommon section titles or organization\n\n7. Document findings and recommendations in `docs/korean_resume_parsing.md`:\n   - Common patterns in Korean resumes\n   - Effective recognition strategies\n   - Challenging cases and solutions\n   - Performance metrics and benchmarks",
        "testStrategy": "1. Quantitative evaluation:\n   - Measure parser accuracy using precision, recall, and F1 scores for:\n     - Section identification (correctly identifying resume sections)\n     - Field extraction (correctly extracting data from each section)\n     - Entity recognition (correctly identifying entities like names, companies)\n   - Track performance metrics over time to ensure improvements\n   - Compare performance across different resume formats and styles\n\n2. Qualitative evaluation:\n   - Conduct manual review of parsing results on the benchmark dataset\n   - Document specific failure cases and improvements\n   - Maintain a catalog of edge cases and their handling\n\n3. Regression testing:\n   - Create automated tests in `tests/evaluation/test_parser_evaluator.py`:\n```python\nimport unittest\nimport os\nimport json\nfrom src.evaluation.parser_evaluator import ParserEvaluator\n\nclass TestParserEvaluator(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.sample_dir = 'tests/fixtures/korean_resumes'\n        cls.ground_truth_dir = 'tests/fixtures/korean_resumes_ground_truth'\n        cls.evaluator = ParserEvaluator(cls.sample_dir, cls.ground_truth_dir)\n    \n    def test_evaluation_metrics(self):\n        results = self.evaluator.evaluate_parser()\n        self.assertGreaterEqual(results['section_f1'].mean(), 0.8, \n                               \"Parser F1 score should be at least 0.8\")\n    \n    def test_benchmark_performance(self):\n        # Test performance on benchmark dataset\n        pass\n```\n\n4. Performance benchmarking:\n   - Measure processing time for different resume types\n   - Identify performance bottlenecks\n   - Ensure the parser can handle the target volume of resumes\n\n5. User acceptance testing:\n   - Have team members review parsing results for a subset of samples\n   - Collect feedback on accuracy and completeness\n   - Identify any missed requirements or edge cases\n\n6. Documentation validation:\n   - Ensure all findings and patterns are properly documented\n   - Verify that the documentation provides clear guidance for future development\n   - Create a final report summarizing the testing process, results, and recommendations",
        "status": "pending",
        "dependencies": [
          11,
          12,
          13
        ],
        "priority": "high",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-04T05:09:38.447Z",
      "updated": "2025-07-04T05:17:43.600Z",
      "description": "Tasks for master context"
    }
  }
}